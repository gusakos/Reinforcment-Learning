{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "cellView": "form",
        "id": "sTOELwyRPA-y"
      },
      "outputs": [],
      "source": [
        "#@title Create arms\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd \n",
        "import random\n",
        "k = 10    #number of arms\n",
        "T = 1000  #horizon  \n",
        "\n",
        "# creating limits for each arm\n",
        "bandits = set()\n",
        "while len(bandits) < 10:\n",
        "    a, b = np.sort(np.random.rand(2))\n",
        "    bandits.add((a, b))\n",
        "bandits = [[x,y] for x, y in bandits]\n",
        "\n",
        "# expected value of each arm\n",
        "expected_val = [np.mean([x,y]) for x, y in bandits]\n",
        "\n",
        "# best arm\n",
        "best = np.amax(expected_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dLmk01NlqR0y"
      },
      "outputs": [],
      "source": [
        "#@title Run UCB\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "u_mi = np.zeros((k,))                   # arm μι estimate\n",
        "u_bandit_score = np.zeros((k,))         #total score of each arm for first N rounds\n",
        "u_pulls = np.zeros((k,))                #num of arm pulls\n",
        "u_inst_score = np.zeros((T,))           #reward for round t\n",
        "u_best_score = np.zeros((T,))           #cumulative reward of best arm for round t\n",
        "u_alg_score = np.zeros((T,))            #cumulative reward for round t\n",
        "u_regret =  np.zeros((T,))              #regret for round t\n",
        "u_times_played = np.zeros((k,))         # num of times armed is played\n",
        "ucb= np.full(k, float(\"inf\"))           # ucb estimate= μi + square_root(lnT/Qi(T))\n",
        "\n",
        "\n",
        "\n",
        "# Exploit arm with the best ucb\n",
        "\n",
        "for i in range(T):\n",
        "\n",
        "  #finding arm with the best ucb index\n",
        "  j= np.argmax(ucb)\n",
        "\n",
        "  #exploiting the arm and store its score\n",
        "  score= np.random.binomial(1,p=bandit[j])\n",
        "  u_inst_score[i] = score\n",
        "  u_bandit_score[j] += score\n",
        "  u_times_played[j] += 1\n",
        "\n",
        "     # maintain estimate μι and ucb of each arm\n",
        "  u_mi[j]= u_bandit_score[j]/u_times_played[j]\n",
        "  ucb[j]= u_mi[j] + np.sqrt(np.log(T)/u_times_played[j])\n",
        "  #print('round= %f arm played %f, ucb=%f, Q affecting ucb=%f'%( i,j,ucb[j], (ucb[j]-mi[j])/ucb[j]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "C15SxyQ7ScZb"
      },
      "outputs": [],
      "source": [
        "#@title Run e-Greedy\n",
        "\n",
        "e_mi = np.zeros((k,))                     # arm estimate\n",
        "e_bandit_score = np.zeros((k,))           #total score of each arm for first N rounds\n",
        "e_pulls = np.zeros((k,))                  #num of arm pulls\n",
        "e_inst_score = np.zeros((T,))             #reward for round t\n",
        "e_best_score = np.zeros((T,))             #cumulative reward of best arm for round t\n",
        "e_alg_score = np.zeros((T,))              #cumulative reward for round t\n",
        "e_regret =  np.zeros((T,))                #regret for round t\n",
        "e_times_played = np.zeros((k,))           #num of times armed is played\n",
        "counter= 0\n",
        "\n",
        "\n",
        "# e-Greedy code\n",
        "for i in range (T):\n",
        "  \n",
        "   # Calculating the exploring probability epsilon= t^(-1/3)\n",
        "   if i==0:\n",
        "     eps=0\n",
        "   else:\n",
        "     eps=np.power(i, -1/3) * np.power((k*np.log2(i)), 1/3)\n",
        "   p= random.random()\n",
        "   \n",
        "   #Choose when to explore or exploit based on an experiment \n",
        "   # Explore with prob epsilon\n",
        "   if p < eps:\n",
        "        j = random.randint(0, len(bandit) - 1)   \n",
        "        \n",
        "   # Exploit best arm  with probability 1-epsilon   \n",
        "   else:\n",
        "        j = np.argmax(e_mi)\n",
        "\n",
        "   # play the arm and store each new score\n",
        "   score= np.random.binomial(1,p=bandit[j])\n",
        "   e_inst_score[i] = score\n",
        "   e_bandit_score[j] += score\n",
        "   e_times_played[j] += 1\n",
        "\n",
        "   # maintain estimate of each arm\n",
        "   e_mi[j]= e_bandit_score[j]/e_times_played[j]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-mONrAvETNFs"
      },
      "outputs": [],
      "source": [
        "#@title Print e-Greedy Cumulatige Regret-T\n",
        "\n",
        "for i in range(k):\n",
        "   print('arm = %d: true mean = %f : estimate mean = %f' % (i,bandit[i],e_mi[i]))\n",
        "\n",
        "for i in range(T):\n",
        "  if i > 0: e_best_score[i] = e_best_score[i-1] + best #vector keeping track of t*optimal reward (cummulative reward)\n",
        "  else: e_best_score[i] = best\n",
        "  if i > 0: e_alg_score[i] = e_alg_score[i-1] + e_inst_score[i] #vector keeping track of cummulative e-Greedy reward at all times \n",
        "  else: e_alg_score[i] = e_inst_score[i]\n",
        "  e_regret[i] = (e_best_score[i] - e_alg_score[i])/(i+1)  #regret per iteration at round t\n",
        "\n",
        "\n",
        "plt.title(\"e-Greedy Performance\") \n",
        "plt.xlabel(\"Round T\") \n",
        "plt.ylabel(\"Total score\") \n",
        "plt.plot(np.arange(1,T+1),e_regret) \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DafHUl_oSXBB"
      },
      "outputs": [],
      "source": [
        "#@title Print UCB Cumulative Regret-T\n",
        "\n",
        "for i in range(k):\n",
        "   print('arm = %d: true mean = %f : estimate mean = %f' % (i,bandit[i],u_mi[i]))\n",
        "\n",
        "\n",
        "for i in range(T):\n",
        "  if i > 0: u_best_score[i] = u_best_score[i-1] + best #vector keeping track of t*optimal reward (cummulative reward)\n",
        "  else: u_best_score[i] = best\n",
        "  if i > 0: u_alg_score[i] = u_alg_score[i-1] + u_inst_score[i] #vector keeping track of UCB reward at all times \n",
        "  else: u_alg_score[i] = u_inst_score[i]\n",
        "  u_regret[i] = (u_best_score[i] - u_alg_score[i])/(i+1)  #regret per iteration at round t\n",
        "\n",
        "plt.title(\"UCB Performance\") \n",
        "plt.xlabel(\"Round T\") \n",
        "plt.ylabel(\"Total score\") \n",
        "plt.plot(np.arange(1,T+1),u_regret) \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qmXfVGAaUUkg"
      },
      "outputs": [],
      "source": [
        "#@title Plot both UCB and e-Greedy regrets\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for i in range(T):\n",
        "  if i > 0: u_best_score[i] = u_best_score[i-1] + best #vector keeping track of t*optimal reward (cummulative reward)\n",
        "  else: u_best_score[i] = best\n",
        "  if i > 0: u_alg_score[i] = u_alg_score[i-1] + u_inst_score[i] #vector keeping track of UCB reward at all times \n",
        "  else: u_alg_score[i] = u_inst_score[i]\n",
        "  u_regret[i] = (u_best_score[i] - u_alg_score[i])/(i+1)  #regret per iteration at round t\n",
        "\n",
        "\n",
        "for i in range(T):\n",
        "  if i > 0: e_best_score[i] = e_best_score[i-1] + best #vector keeping track of t*optimal reward (cummulative reward)\n",
        "  else: e_best_score[i] = best\n",
        "  if i > 0: e_alg_score[i] = e_alg_score[i-1] + e_inst_score[i] #vector keeping track of cummulative e-Greedy reward at all times \n",
        "  else: e_alg_score[i] = e_inst_score[i]\n",
        "  e_regret[i] = (e_best_score[i] - e_alg_score[i])/(i+1)  #regret per iteration at round t\n",
        "\n",
        "\n",
        "plt.title(\"ε-Greedy vs UCB Performance\")\n",
        "plt.xlabel(\"Round T\")\n",
        "plt.ylabel(\"Total score\")\n",
        "plt.plot(np.arange(1,T+1),u_regret, label=\"UCB\") \n",
        "plt.plot(np.arange(1,T+1),e_regret, label=\"ε-Greedy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
